"""Test of the implementation of a sequential network w/ dropout"""

from typing import Optional

import numpy as np

from amllib.networks import Sequential
from amllib.layers import Dense, Dropout
from amllib.activations import ReLU, Linear, Logistic, SoftMax, TanH
from amllib.utils import mnist
from amllib.optimizers import SGD, Adam, Optimizer
from amllib.regularizers import L2Regularizer

def get_overfitting_model() -> Sequential:
    """
    Generate a feedforward network which overfits on a
    part of the MNIST dataset.

    Returns
    -------
    Sequential
        Untrained overfitting FNN
    """

    model = Sequential(input_shape=(784,))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dense(10, afun=SoftMax(), optim=Adam()))

    return model

def get_dropout_model(p: float) -> Sequential:
    """
    Generate a feedforward network with the same structure
    as the network generated by `get_overfitting_model`,
    but with dropout in the hidden layers.

    Parameters
    ----------
    p : float
        Dropout probability for each hidden layer.

    Returns
    -------
    Sequential
        Untrained FNN using dropout.
    """

    model = Sequential(input_shape=(784,))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dropout(p=p))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dropout(p=p))
    model.add_layer(
        Dense(512, afun=ReLU(), optim=Adam()))
    model.add_layer(
        Dropout(p=p))
    model.add_layer(
        Dense(10, afun=SoftMax(), optim=Adam()))

    return model

def evaluate_model(model, x_train, y_train, x_test, y_test):

    y_tilde = model(x_train)
    prod = mnist.encode_labels(y_train) * np.log(y_tilde)
    loss = -np.sum(np.sum(prod, axis=1)) / y_tilde.shape[0]

    y_tilde = np.argmax(y_tilde, axis=1)
    accuracy = np.sum(y_tilde == y_train) / y_tilde.size

    print(f'Training loss: {loss:3.2e}')
    print(f'Training accuracy: {(accuracy * 100):5.2f}%')

    y_tilde = model(x_test)
    prod = mnist.encode_labels(y_test) * np.log(y_tilde)
    loss = -np.sum(np.sum(prod, axis=1)) / y_tilde.shape[0]

    y_tilde = np.argmax(y_tilde, axis=1)
    accuracy = np.sum(y_tilde == y_test) / y_tilde.size
    print(f'Test loss: {loss:3.2e}')
    print(f'Test accuracy: {(accuracy * 100):5.2f}%')

def test_dropout():

    # Load MNIST dataset
    x_train, y_train, x_test, y_test = mnist.load_data()
    x_train = mnist.flatten_input(x_train)
    x_test = mnist.flatten_input(x_test)
    y = mnist.encode_labels(y_train)

    # Reduce the dataset to encourage overfitting.
    x_train = x_train[:8192, :]
    y = y[:8192, :]
    y_train = y_train[:8192]

    print('--------------------------------------------')
    print('TESTING OVERFITTING MODEL')

    model = get_overfitting_model()
    model.train(x_train, y, batch_size=128, epochs=10)
    evaluate_model(model, x_train, y_train, x_test, y_test)

    for p in [.1, .2, .3, .5, .7]:
        print('--------------------------------------------')
        print(f'TESTING DROPOUT MODEL, p = {p}')

        model = get_dropout_model(p)
        model.train(x_train, y, batch_size=128, epochs=20)
        evaluate_model(model, x_train, y_train, x_test, y_test)

if __name__ == '__main__':

    test_dropout()
